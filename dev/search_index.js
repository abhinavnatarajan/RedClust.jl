var documenterSearchIndex = {"docs":
[{"location":"changelog/#Changelog","page":"Changelog","title":"Changelog","text":"","category":"section"},{"location":"changelog/#[1.0.0]","page":"Changelog","title":"[1.0.0]","text":"","category":"section"},{"location":"changelog/#Added","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"examples from the main paper are now included in the examples folder. ","category":"page"},{"location":"changelog/#Removed","page":"Changelog","title":"Removed","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"dependency on RCall and the various calls to the salso algorithm were removed. It is left to the user to make these calls if necessary. ","category":"page"},{"location":"changelog/#[0.2.2]","page":"Changelog","title":"[0.2.2]","text":"","category":"section"},{"location":"changelog/#Fixed","page":"Changelog","title":"Fixed","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"corrected time not printing in the correct format. \nmoved prettytime and prettynumber to utils.jl.","category":"page"},{"location":"changelog/#[0.2.1]","page":"Changelog","title":"[0.2.1]","text":"","category":"section"},{"location":"changelog/#Added-2","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"added tests for pretty printing.\nadded tests for custom loss function in getpointestimate. ","category":"page"},{"location":"changelog/#Fixed-2","page":"Changelog","title":"Fixed","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"verbose output in sampler missing newline. \ncustom loss function when computing a point estimate. ","category":"page"},{"location":"changelog/#Removed-2","page":"Changelog","title":"Removed","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"_infodist not in use, removed.","category":"page"},{"location":"changelog/#[0.2.0]","page":"Changelog","title":"[0.2.0]","text":"","category":"section"},{"location":"changelog/#Added-3","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"added log-posterior to result.\nadded log-likelihood and log-posterior plots to basic example.\npoint-estimation via maximum likelihood and maximum a posteriori.\ninfodist function to compute the information distance between clusterings.\nconvenience constructor for MCMCData.\nadd verbose option for runsampler and fitprior.\npretty printing for MCMCData, MCMCOptionsList, and PriorHyperparamsList.\nadded bibliographic references to documentation.\nadded changelog.","category":"page"},{"location":"changelog/#Breaking","page":"Changelog","title":"Breaking","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"MCMCOptions constructor changed account for change in point-estimate calculation. \nfitprior now only returns the hyperparameter list.\nremoved summarise for MCMCResult objects (use pretty printing instead).","category":"page"},{"location":"changelog/#Fixed-3","page":"Changelog","title":"Fixed","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"corrected computation of log-likelihood in result.\nfixed edge case error in fitprior when the found value of K is either 1 or N. This case arises only for edge values of Kmin and Kmax, since the elbow method will otherwise not choose 1 or N for K.","category":"page"},{"location":"changelog/#Changed","page":"Changelog","title":"Changed","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"added bounds checking for Kmin and Kmax in fitprior.\nadded input message for better debugging in fitprior.\nadded progress bar for fitprior if useR = false. \nadded input validation for generatemixture.\nadded input validation for binderloss and evaluateclustering.\nadded input validation for the constructors of MCMCData and MCMCOptionsList.\nseparated computation of log-likelihood and log-prior.\nremoved redundant loss function calculations when computing point-estimate.\nBinder loss calculation (binderloss) is now approximate (using randindex from Clustering.jl) but faster.\nrunsampler no longer automatically calculates a point estimate. \nsummarise has a separate signature for MCMC output and for point estimate summary, and now provides more measures for point estimates.\nminor optimisations using @inbounds, @simd, and @turbo.\nusing StaticArrays.jl and separate function for restricted Gibbs scan. \nspeedup via non-generic implementation of matrix and vector sums with LoopVectorization.jl.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Main-functions","page":"Reference","title":"Main functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [RedClust]\nPages   = [\"prior.jl\", \"mcmc.jl\"]\nOrder = [:function]","category":"page"},{"location":"reference/#RedClust.fitprior","page":"Reference","title":"RedClust.fitprior","text":"fitprior(data, algo, diss = false; \nKmin = 1, \nKmax = Int(floor(size(data)[end] / 2), \nverbose = true)\n\nDetermines the best prior hyperparameters from the data. A notional clustering is obtained using k-means or k-medoids, and the distances are split into within-cluster distances and inter-cluster distances based on the notional clustering. These distances are then used to fit the prior hyperparameters using MLE and empirical Bayes sampling.   \n\nArguments\n\ndata::Union{Vector{Vector{Float64}}, Matrix{Float64}}: can either be a vector of (possibly multi-dimensional) observations, or a matrix with each column an observation, or a square matrix of pairwise dissimilarities. \nalgo::String: must be one of \"k-means\" or \"k-medoids\".\ndiss::bool: if true, data will be assumed to be a pairwise dissimilarity matrix. \nKmin::Integer: minimum number of clusters.\nKmax::Integer: maximum number of clusters. If left unspecified, it is set to half the number of observations.\nverbose::Bool: if false, disables all info messages and progress bars. \n\nReturns\n\nAn object of type PriorHyperparamsList.\n\n\n\n\n\n","category":"function"},{"location":"reference/#RedClust.runsampler","page":"Reference","title":"RedClust.runsampler","text":"runsampler(data, \noptions = MCMCOptionsList(), \nparams = PriorHyperparamsList(); \nverbose = true) -> MCMCResult\n\nRuns the MCMC sampler on the data.\n\nArguments\n\ndata::MCMCData: contains the distance matrix. \noptions::MCMCOptionsList: contains the number of iterations, burnin, etc. \nparams::PriorHyperparamsList: contains the prior hyperparameters for the model.\nverbose::Bool: if false, disables all info messages and progress bars. \n\nReturns\n\nA struct of type MCMCResult containing the MCMC samples, convergence diagnostics, and summary statistics.\n\nSee also\n\nMCMCData, MCMCOptionsList, fitprior, PriorHyperparamsList, MCMCResult.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Point-estimation","page":"Reference","title":"Point estimation","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [RedClust]\nPages   = [\"pointestimate.jl\"]\nOrder = [:function]","category":"page"},{"location":"reference/#RedClust.binderloss-Tuple{Vector{Int64}, Vector{Int64}}","page":"Reference","title":"RedClust.binderloss","text":"binderloss(a::ClustLabelVector, b::ClustLabelVector;\nnormalised = true) -> Float64\n\nComputes the Binder loss between two clusterings. If normalised = true then the result is equal to one minus the rand index between a and b. \n\n\n\n\n\n","category":"method"},{"location":"reference/#RedClust.getpointestimate-Tuple{MCMCResult}","page":"Reference","title":"RedClust.getpointestimate","text":"getpointestimate(samples::MCMCResult; method = \"MAP\", loss = \"VI\")\n\nComputes a point estimate from a vector of samples of cluster allocations by searching for a minimiser of the posterior expectation of some loss function. \n\nArguments\n\nmethod::String: must be one of the following -\n\"MAP\": maximum a posteriori.\n\"MLE\": maximum likelihood estimation.\n\"MPEL\": minimum posterior expected loss.\n\"MAP\" and \"MLE\" search among the MCMC samples for the clustering with the maximum log posterior and log likelihood respectively. \"MPEL\" searches for a clustering that minimises the posterior expected loss of some loss function specified by the loss argument. The search space is the set of samples in samples. \nloss: Determines the loss function used for the \"MPEL\" method. Must be either be a string or a function. If specified as a string, must be one of \"binder\" (Binder loss), \"omARI\" (one minus the Adjusted Rand Index), \"VI\" (Variation of Information distance), or \"ID\" (Information Distance). If specified as a function, must have a method defined for (x::Vector{Int}, y::Vector{Int}) -> Real. \n\nReturns\n\nReturns a tuple (clust, i) where clust is a clustering in samples and i is its sample index. \n\n\n\n\n\n","category":"method"},{"location":"reference/#RedClust.infodist-Tuple{Vector{Int64}, Vector{Int64}}","page":"Reference","title":"RedClust.infodist","text":"infodist(a::ClustLabelVector, b::ClustLabelVector;\nnormalised = true) -> Float64\n\nComputes the information distance between two clusterings. The information distance is defined as \n\nd_mathrmID(a b) = max H(A) H(B) - I(A B)\n\nwhere A and B are the cluster membership probability functions for a and b respectively, H denotes the entropy of a distribution, and I denotes the mutual information between two distributions. The information distance has range 0 log N where N is the number of observations. If normalised = true, the result is scaled to the range 0 1.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Summary-and-clustering-evaluation","page":"Reference","title":"Summary and clustering evaluation","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [RedClust]\nPages   = [\"summaries.jl\"]\nOrder = [:function]","category":"page"},{"location":"reference/#RedClust.evaluateclustering-Tuple{Vector{Int64}, Vector{Int64}}","page":"Reference","title":"RedClust.evaluateclustering","text":"evaluateclustering(clusts::ClustLabelVector, truth::ClustLabelVector)\n\nReturns a named tuple of values quantifying the accuracy of the clustering assignments in clusts with respect to the ground truth clustering assignments in truth. \n\nReturn values\n\nnbloss: Normalised Binder loss (= 1 - rand index). Lower is better. \nari: Adjusted Rand index. Higher is better. \nvi: Variation of Information distance (in 0 log N). Lower is better.\nnvi: Normalised VI distance (in 0 1).\nid: Information Distance (in 0 log N). Lower is better. \nnid: Normalised Information Distance (in 0 1).\nnmi: Normalised Mutual Information (in 0 1). Higher is better. \n\n\n\n\n\n","category":"method"},{"location":"reference/#RedClust.summarise-Tuple{IO, Vector{Int64}, Vector{Int64}}","page":"Reference","title":"RedClust.summarise","text":"summarise([io::IO], clusts::ClustLabelVector, \ntruth::ClustLabelVector, \nprintoutput = true) -> String\n\nPrints a summary of the clustering accuracy of clusts with respect to the ground truth in truth. The output is printed to the output stream io, which defaults to stdout if not provided.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Convenience-functions","page":"Reference","title":"Convenience functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [RedClust]\nPages   = [\"utils.jl\"]\nOrder = [:function]","category":"page"},{"location":"reference/#RedClust.adjacencymatrix-Tuple{Vector{Int64}}","page":"Reference","title":"RedClust.adjacencymatrix","text":"adjacencymatrix(clusts::ClustLabelVector) -> Matrix{Bool}\n\nReturns the n×n adjacency matrix corresponding to the given cluster label vector clusts, where n = length(clusts). \n\n\n\n\n\n","category":"method"},{"location":"reference/#RedClust.generatemixture-Tuple{Integer, Integer}","page":"Reference","title":"RedClust.generatemixture","text":"generatemixture(N, K; α = K, dim = K, radius = 1, σ = 0.1)\n\nGenerates a multivariate Normal mixture, with kernel weights generated from a Dirichlet prior. The kernels are centred at the vertices of a dim-dimensional simplex with edge length radius.\n\nArguments\n\nN::Integer: number of observations to generate.\nK::Integer: number of mixture kernels.\nα::Float64 = K: parameter for the Dirichlet prior.\ndim::Integer = K: dimension of the observations.\nradius::Float64 = 1: radius of the simplex whose vertices are the kernel means.\nσ::Float64 = 0.1: variance of each kernel.\n\nReturns\n\nNamed tuple containing the following fields-\n\npoints::Vector{Vector{Float64}}: a vector of N observations.\ndistancematrix::Matrix{Float64}: an N×N matrix of pairwise Euclidean distances between the observations.\nclusts::ClustLabelVector: vector of N cluster assignments.\nprobs::Float64: vector of K cluster weights generated from the Dirichlet prior, used to generate the observations.\noracle_coclustering::Matrix{Float64}: N×N matrix of co-clustering probabilities, calculated assuming full knowledge of the cluster centres and cluster weights.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RedClust.makematrix-Tuple{AbstractVector{<:AbstractVector}}","page":"Reference","title":"RedClust.makematrix","text":"makematrix(x::AbstractVector{<:AbstractVector}) -> Matrix\n\nConvert a vector of vectors into a matrix, where each vector becomes a column in the matrix.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RedClust.sortlabels-Tuple{Vector{Int64}}","page":"Reference","title":"RedClust.sortlabels","text":"sortlabels(x::ClustLabelVector) -> ClustLabelVector\n\nReturns a cluster label vector y such that x and y have the same adjacency structure and labels in y occur in sorted ascending order.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Types","page":"Reference","title":"Types","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [RedClust]\nOrder = [:type]","category":"page"},{"location":"reference/#RedClust.ClustLabelVector","page":"Reference","title":"RedClust.ClustLabelVector","text":"Alias for Vector{Int}\n\n\n\n\n\n","category":"type"},{"location":"reference/#RedClust.MCMCData","page":"Reference","title":"RedClust.MCMCData","text":"MCMCData(points::AbstractVector{<:AbstractVector{<:Float64}})    \nMCMCData(dissimilaritymatrix::AbstractMatrix{Float64})\n\nContains the pairwise dissimilarities for the MCMC sampler.  \n\n\n\n\n\n","category":"type"},{"location":"reference/#RedClust.MCMCOptionsList","page":"Reference","title":"RedClust.MCMCOptionsList","text":"MCMCOptionsList(; \nnumiters = 5000, \nburnin = floor(0.2 * numiters), \nthin = 1, \nnumGibbs = 5, \nnumMH = 1)\n\nList of options for running the MCMC. \n\nConstructor arguments\n\nnumiters::Integer: number of iterations to run.\nburnin::Integer: number of iterations to discard as burn-in.\nthin::Integer: will keep every thin samples.\nnumGibbs:Integer: number of intermediate Gibbs scans in the split-merge step.\nnumMH:Integer: number of split-merge steps per MCMC iteration.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RedClust.MCMCResult","page":"Reference","title":"RedClust.MCMCResult","text":"Struct containing MCMC samples. \n\nFields\n\noptions::MCMCOptionsList: options passed to the sampler. \nparams::PriorHyperparamsList: prior hyperparameters used by the sampler. \nclusts::Vector{ClustLabelVector}: contains the clustering allocations. clusts[i] is a vector containing the clustering allocation from the ith sample. \nposterior_coclustering::Matrix{Float64}: the posterior coclustering matrix. \nK::Vector{Int}: posterior samples of K, i.e., the number of clusters. K[i] is the number of clusters in clusts[i].\nr::Vector{Float64}: posterior samples of the parameter r.\np::Vector{Float64}: posterior samples of the parameter p.\nK_mean, r_mean, p_mean: posterior mean of K, r, and p respectively. \nK_variance, r_variance, p_variance: posterior variance of K, r, and p respectively. \nK_acf::Vector{Float64}, r_acf::Vector{Float64}, p_acf::Vector{Float64}: autocorrelation function for K, r, and p respectively. \nK_iac, r_iac, and p_iac: integrated autocorrelation coefficient for K, r, and p respectively. \nK_ess::Float64, r_ess::Float64, and p_ess::Float64: effective sample size for K, r, and p respectively.\nloglik::Vector{Float64}: log-likelihood for each sample. \nlogposterior::Vector{Float64}: a function proportional to the log-posterior for each sample, with constant of proportionality equal to the normalising constant of the partition prior.\nsplitmerge_splits: Boolean vector indicating the iterations when a split proposal was used in the split-merge step. Has length numMH * numiters (see MCMCOptionsList).\nsplitmerge_acceptance_rate: acceptance rate of the split-merge proposals. \nr_acceptances: Boolean vector indicating the iterations (including burnin and the thinned out iterations) where the Metropolis-Hastings proposal for r was accepted. \nr_acceptance_rate:: Metropolis-Hastings acceptance rate for r.\nruntime: total runtime for all iterations.\nmean_iter_time: average time taken for each iteration. \n\n\n\n\n\n","category":"type"},{"location":"reference/#RedClust.PriorHyperparamsList","page":"Reference","title":"RedClust.PriorHyperparamsList","text":"PriorHyperparamsList(; [kwargs])\n\nContains the prior hyperparameters for the model. \n\nConstructor arguments\n\nδ1::Float64 = 1: the parameter delta_1.\nα::Float64 = 1: the shape parameter alpha in the prior for each lambda_k.\nβ::Float64 = 1: the rate parameter beta in the prior for each lambda_k.\nδ2::Float64 = 1: the parameter delta_2.\nζ::Float64 = 1: the shape parameter zeta in the prior for each theta_kt.\nγ::Float64 = 1: the rate parameter gamma in the prior for each theta_kt.\nη::Float64 = 1: the shape parameter eta in the prior for r.\nσ::Float64 = 1: the rate parameter sigma in the prior for r.\nu::Float64 = 1: the parameter u in the prior for p.\nv::Float64 = 1: the parameter v in the prior for p.\nproposalsd_r::Float64 = 1: standard deviation of the truncated Normal proposal when sampling r via a Metropolis-Hastings step. \nK_initial::Int = 1: initial number of clusters to start the MCMC.\nrepulsion::Bool = true: whether to use the repulsive component of the likelihood. \nmaxK::Int = 0: maxmimum number of clusters to allow. If set to 0 then no maximum is imposed.\n\nSee also\n\nfitprior\n\n\n\n\n\n","category":"type"},{"location":"","page":"Introduction","title":"Introduction","text":"CurrentModule = RedClust","category":"page"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"RedClust is a Julia package for Bayesian clustering of high-dimensional Euclidean data using pairwise dissimilarity information instead of the raw observations. It uses an MCMC sampler to generate posterior samples from the space of all possible clustering structures on the data. ","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The package can be installed by typing ]add RedClust into the Julia REPL or by the usual method:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using Pkg\nPkg.add(\"RedClust\")","category":"page"},{"location":"#Basic-example","page":"Introduction","title":"Basic example","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"using RedClust\n# Generate data\npoints, distM, clusts, probs, oracle_coclustering = \n\tgeneratemixture(N, K; α = 10, σ = data_σ, dim = data_dim)\n# Let RedClust choose the best prior hyperparameters\nparams = fitprior(pnts, \"k-means\", false)\n# Set the MCMC options\noptions = MCMCOptionsList(numiters = 5000)\ndata = MCMCData(points)\n# Run the sampler\nresult = runsampler(data, options, params)\n# Get a point estimate \npointestimate, index = getpointestimate(result)\n# Summary of MCMC and point estimate\nsummarise(result)\nsummarise(pointestimate, clusts)","category":"page"},{"location":"#Model","page":"Introduction","title":"Model","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"RedClust implements the model described in Natarajan et al. (2022). The key features are-","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The use of a random partition model with an unknown number of clusters K which allows for posterior inference on K. That is, there is a prior distribution on the space of all possible clustering structures with any number of clusters from one to the number of observations. The number of clusters is an object of inference to be determined by MCMC sampling. \nThe pairwise dissimilarities between observations are assumed to be mutually independent given the clustering assignment; that is, the clustering likelihood is a composite or pseduo-likelihood. \nThe clustering likelihood is comprised of a cohesive part and a repulsive part. The repulsive component of the likelihood imposes a strong identifiability constraint on the clustering; clusters must not only comprise points that have similar small distances among themselves, but also similar distances to points in other clusters.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The prior on the clustering structure can be chosen according to application in question. The current version of RedClust implements a microclustering prior (Miller et al., 2015; Betancourt et al., 2022). This means that the partition is generated by drawing cluster sizes n_1 ldots n_K from a random distribution nu (conditional upon n_1 + ldots n_K = n where n is the number of observations), and cluster labels are given by a uniform random permutation of ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"underbrace1 ldots 1_n_1 text times ldots underbraceK ldots K_n_K text times","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"RedClust implements a microclustering prior with nu a shifted negative binomial with random parameters r and p, which are sampled from a Gamma and Beta distribution respectively.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"beginalign*\nr sim mathrmGamma(eta sigma)\np sim mathrmBeta(u v)\nendalign*","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This results in the following partition prior: ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"pi(rho_n mid r p) propto K p^n-K(1-p)^rKGamma(r)^-Kprod_k=1^K n_k Gamma(n_k+r-1)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"where rho_n is the partition and K is the number of clusters in the partition. The partition likelihood is given by","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"beginalign*\nlambda_k oversetmathrmiidsim mathrmGamma(alpha beta) quad 1 leq k leq K\ntheta_kt oversetmathrmiidsim mathrmGamma(zeta gamma) quad 1 leq k  t leq K\nendalign*","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"pi(D mid rho_n boldsymbollambda boldsymboltheta r p) = left prod_k=1^K prod_substacki j in C_k  i neq j fracD_ij^delta_1 - 1lambda_k^delta_1Gamma(delta_1) exp(-lambda_k D_ij) right leftprod_substackt k = 1  t neq K prod_substacki in C_k  j in C_t fracD_ij^delta_2-1theta_kt^delta_2Gamma(delta_2) exp(-theta_ktD_ij) right","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"where boldsymbol D is the matrix of pairwise dissimilarities between observations.","category":"page"},{"location":"#Point-estimation","page":"Introduction","title":"Point estimation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"A clustering point-estimate boldsymbol c^* can be determined in a number of ways. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Choosing the sample with maximum likelihood or maximum posterior. This corresponds to an MLE or MAP estimate using the MCMC samples as an approximation to the likelihood and posterior.\nSearching for a clustering that minimises the posterior expectation of a loss function ell such as the Binder loss (Binder, 1978), the Variation of Information distance (Meilă, 2007; Wade and Ghahramani, 2018), or the Normalised Information Distance (Kraskov et al., 2005). That is, ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"boldsymbol c^* = argmin_boldsymbol c mathbbE_boldsymbol cell(boldsymbol c boldsymbol c)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"A naive method is to restrict the search space to those clusterings visited by the MCMC sampler. This method is implemented in RedClust in the function getpointestimate.\nA better method is the SALSO algorithm (Dahl et al., 2022), implemented in the R package salso, which heuristically searches the space of all possible clusterings. You can use the RCall package in Julia to make calls to the salso package in R if you have R and salso installed.  ","category":"page"},{"location":"#Citing-this-package","page":"Introduction","title":"Citing this package","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"If you want to use this package in your work, please cite it as:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Natarajan, A., De Iorio, M., Heinecke, A., Mayer, E. and Glenn, S., 2022. ‘Cohesion and Repulsion in Bayesian Distance Clustering’, arXiv 2107.05414.","category":"page"},{"location":"#Bibliography","page":"Introduction","title":"Bibliography","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"<a id=\"betancourt\"></a>","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Betancourt, B., Zanella, G. and Steorts, R. C. (2022), ‘Random partition models for microclustering tasks’, Journal of the American Statistical Association 117(539), 1215–1227. DOI: 10.1080/01621459.2020.1841647.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"<a id=\"binder\"></a>","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Binder, D. A. (1978). Bayesian Cluster Analysis. Biometrika, 65(1), 31–38. DOI: 10.2307/2335273.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"<a id=\"dahl\"></a>","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Dahl, D. B., Johnson, D. J. and M¨uller, P. (2022), ‘Search algorithms and loss functions for Bayesian clustering’, Journal of Computational and Graphical Statistics 0(0), 1–13. DOI: 10.1080/10618600.2022.2069779.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"<a id=\"kraskov\"></a>","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Kraskov, A., Stögbauer, H., Andrzejak, R. G. and P Grassberger, P. (2005), ‘Hierarchical clustering using mutual information’, Europhysics Letters (EPL) 70(2), 278-284, DOI: 10.1209/epl/i2004-10483-y.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"<a id=\"meila\"></a>","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Marina Meilă (2007), ‘Comparing clusterings—an information based distance’, Journal of Multivariate Analysis,  98(5), 873-895, DOI: 10.1016/j.jmva.2006.11.013.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"<a id=\"miller\"></a>","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Miller, J., Betancourt, B., Zaidi, A., Wallach, H. and Steorts, R. C. (2015), ‘Microclustering: When the cluster sizes grow sublinearly with the size of the data set’, arXiv 1512.00792.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"<a id=\"wade\"></a>","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Wade, S. and Ghahramani, Z. (2018), ‘Bayesian Cluster Analysis: Point Estimation and Credible Balls (with Discussion)’, Bayesian Analysis 13(2), 559 – 626. DOI: 10.1214/17-BA1073.","category":"page"}]
}
