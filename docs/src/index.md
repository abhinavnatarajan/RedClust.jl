```@meta
CurrentModule = RedClust
```

# Introduction

[RedClust](https://github.com/abhinavnatarajan/RedClust.jl) is a [Julia](https://julialang.org/) package for Bayesian clustering of high-dimensional Euclidean data using pairwise dissimilarity information instead of the raw observations. It uses an MCMC sampler to generate posterior samples from the space of all possible clustering structures on the data. 

## Installation
The package can be installed by typing `]add RedClust` into the Julia REPL or by the usual method:
```julia
using Pkg
Pkg.add("RedClust")
```
RedClust also requires [`R`](https://www.r-project.org/) and the R package [`salso`](https://CRAN.R-project.org/package=salso). If R is already installed, make sure the `R_HOME` environment variable is set to the R home directory (you could run `R.home()` in R to determine the location of this directory). If R or `salso` are not found, they are automatically installed during package installation.  

## Basic example
```julia
using RedClust
# Generate data
pnts, distM, clusts, probs, oracle_coclustering = 
	generatemixture(N, K; α = 10, σ = data_σ, dim = data_dim)
# Let RedClust choose the best prior hyperparameters
params = fitprior(pnts, "k-means", false).params
# Set the MCMC options
options = MCMCOptionsList(numiters = 5000)
data = MCMCData(D = distM)
# Run the sampler
result = runsampler(data, options, params)
```

## Model
RedClust implements the model described in Natarajan et al. (2022). The key features are-
1. The use of a random partition model with an unknown number of clusters ``K`` which allows for posterior inference on ``K``. That is, there is a prior distribution on the space of all possible clustering structures with any number of clusters from one to the number of observations. The number of clusters is an object of inference to be determined by MCMC sampling. 
2. The pairwise dissimilarities between observations are assumed to be mutually independent given the clustering assignment; that is, the clustering likelihood is a *composite* or *pseduo-likelihood*. 
3. The clustering likelihood is comprised of a *cohesive* part and a *repulsive* part. 

The prior on the clustering structure can be chosen according to application in question. The current version of RedClust implements a *microclustering* prior (Miller et al., 2015; Betancourt et al., 2022). This means that the partition is generated by drawing cluster sizes ``n_1, \ldots, n_K`` from a random distribution ``\nu`` (conditional upon ``n_1 + \ldots n_K = n`` where ``n`` is the number of observations), and cluster labels are given by a uniform random permutation of 
```math
\underbrace{1, \ldots, 1}_{n_1 \text{ times}}, \ldots, \underbrace{K, \ldots, K}_{n_K \text{ times}}
```
RedClust implements a microclustering prior with ``\nu`` a shifted negative binomial with random parameters ``r`` and ``p``. 
```math
\begin{align*}
r &\sim \mathrm{Gamma}(\eta, \sigma)\\
p &\sim \mathrm{Beta}(u, v)\\
\pi(\rho_n \mid r, p) &\propto K! p^{n-K}(1-p)^{rK}\Gamma(r)^{-K}\prod_{k=1}^K n_k \Gamma(n_k+r-1)
\end{align*}
```
where ``\rho_n`` is the partition and ``K`` is the number of clusters in the partition. The partition likelihood is given by
```math
\begin{align*}
\lambda_k &\overset{\mathrm{iid}}{\sim} \mathrm{Gamma}(\alpha, \beta), \quad 1 \leq k \leq K\\
\theta_{kt} &\overset{\mathrm{iid}}{\sim} \mathrm{Gamma}(\zeta, \gamma), \quad 1 \leq k < t \leq K
\end{align*}
```
```math
\pi(D, \mid \rho_n, \boldsymbol{\lambda}, \boldsymbol{\theta}, r, p) = \left[ \prod_{k=1}^K \prod_{\substack{i, j \in C_k \\ i \neq j}} \frac{D_{ij}^{\delta_1 - 1}\lambda_k^{\delta_1}}{\Gamma(\delta_1)} \exp(-\lambda_k D_{ij}) \right] \left[\prod_{\substack{t, k = 1 \\ t \neq K}} \prod_{\substack{i \in C_k \\ j \in C_t}} \frac{D_{ij}^{\delta_2-1}\theta_{kt}^{\delta_2}}{\Gamma(\delta_2)} \exp(-\theta_{kt}D_{ij}) \right]
```
where ``\boldsymbol D`` is the matrix of pairwise dissimilarities between observations.

## Point estimation
A clustering point-estimate ``\boldsymbol c^*`` can be determined by searching for a clustering that minimises the expectation of the Variation of Information distance (Wade and Ghahramani, 2018) from a clustering ``\boldsymbol c'`` chosen randomly from the posterior distribution. That is, 
```math
\boldsymbol c^* = \argmin_{\boldsymbol c} \mathbb{E}_{\boldsymbol c'}[d_{mathrm{VI}}(\boldsymbol c, \boldsymbol c')]
```
1. A naive method is to restrict the search space to those clusterings visited by the MCMC sampler. 
2. A better method is the SALSO algorithm (Dahl et al., 2022), implemented in the [R](https://www.r-project.org/) package [salso](https://CRAN.R-project.org/package=salso). 
The MCMC sampler in RedClust automatically computes a point estimate using one of the methods above. The choice of method can be specified in the options passed to the sampler. See [`MCMCOptionsList`](@ref).

## Citing this package
If you want to use this package in your work, please cite it as:

Natarajan, A., De Iorio, M., Heinecke, A., Mayer, E. and Glenn, S., 2022. ‘Cohesion and Repulsion in Bayesian Distance Clustering’, arXiv [2107.05414](https://arxiv.org/abs/2107.05414).

## Bibliography
Betancourt, B., Zanella, G. and Steorts, R. C. (2022), ‘Random partition models for microclustering
tasks’, _Journal of the American Statistical Association_ 117(539), 1215–1227. DOI: [10.1080/01621459.2020.1841647](https://doi.org/10.1080/01621459.2020.1841647).

Dahl, D. B., Johnson, D. J. and M¨uller, P. (2022), ‘Search algorithms and loss functions
for bayesian clustering’, _Journal of Computational and Graphical Statistics_ 0(0), 1–13. DOI: [10.1080/10618600.2022.2069779](https://doi.org/10.1080/10618600.2022.2069779).

Miller, J., Betancourt, B., Zaidi, A., Wallach, H. and Steorts, R. C. (2015), ‘Microclustering:
When the cluster sizes grow sublinearly with the size of the data set’, arXiv
[1512.00792](https://arxiv.org/abs/1512.00792).

Wade, S. and Ghahramani, Z. (2018), ‘Bayesian Cluster Analysis: Point Estimation and
Credible Balls (with Discussion)’, _Bayesian Analysis_ **13**(2), 559 – 626. DOI: [10.1214/17-BA1073](https://doi.org/10.1214/17-BA1073).



